# -*- coding: utf-8 -*-
"""research

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UPDhnCfZ4g3cuT2qj00B6WPkmY_Fkkp
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia google-generativeai google-genai langchain_google_genai

import os
import getpass
import dotenv
dotenv.load_dotenv()
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro-latest",  # ‚úÖ or any other from model list
    temperature=0.7  # You can adjust this
)

res= llm.invoke("test")
res.content

res= llm.invoke("True if the user's question requires external search like browser to answer, False otherwise. : what is pervious question i asked ")
res.content

from typing import List
from typing_extensions import TypedDict
from langgraph.graph import START, MessagesState, StateGraph
from pydantic import BaseModel, Field
import operator
from typing import Annotated



class SearchDecision(BaseModel):
    needs_search: bool = Field(
        description="True if the user's question requires external search to answer, False otherwise."
    )

class Researchstate(MessagesState):
  question:str
  answer:str
  context: Annotated[list, operator.add]
  needs_search: bool

from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.messages import AIMessage
search_classifier_prompt = [
    SystemMessage(content="""
You are a smart research agent helper.

Your job is to determine whether a user's question needs external research.

If the question asks about:
- Recent events
- Unknown facts
- Definitions or explanations not previously mentioned

‚Üí Return: `needs_search = true`

If the question is about:
- Previous conversation (e.g. "What did I just ask?")
- Greetings or small talk (e.g. "what's up?")
- Reflections or meta-questions

‚Üí Return: `needs_search = false`
""")
]


def check(state):
  question = state["question"]
  decision_model = llm.with_structured_output(SearchDecision)
  decision = decision_model.invoke(search_classifier_prompt + [HumanMessage(content=question)])
  return {"needs_search": decision.needs_search,
          "messages": state["messages"]
}

import os
os.environ["TAVILY_API_KEY"] = "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"

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import WikipediaLoader
tavily_search = TavilySearchResults(max_results=3)

def search_web(state):
  """retrives docs from web search """
  tavily_search=TavilySearchResults(max_results=3)
  search_docs= tavily_search.invoke(state["question"])

  formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document href="{doc["url"]}">\n{doc["content"]}\n</Document>'
            for doc in search_docs
        ]
    )
  return {"context":[formatted_search_docs]}

def search_wiki(state):
  """retrives docs from wiki search """
  search_docs= WikipediaLoader( query= state["question"],load_max_docs=2).load()
  formatted_search_docs = "\n\n---\n\n".join(
        [
            f'<Document source="{doc.metadata["source"]}" page="{doc.metadata.get("page", "")}">\n{doc.page_content}\n</Document>'
            for doc in search_docs
        ]
    )
  return {"context":[formatted_search_docs]}

def generate_ans(state):
  """node to answer a question """
  context= state["context"]
  question= state["question"]
  needs_search= state["needs_search"]
  messages = state.get("messages", [])
  print("\n--- DEBUG INFO ---")
  print("Question:", question)
  print("Needs Search:", needs_search)
  print("Context (type):", type(context))
  # print("Context:", context if context else "[EMPTY]")
  print("Messages (before):", [m.content for m in messages])

  # Initialize variables for streaming
  full_response = ""

  if needs_search:
    print("üîç Found relevant information, generating answer...")
    system_message = SystemMessage(content=f"Answer the question {question} using this context: {context}")
    messages = messages+[system_message]
    
    # Stream the response
    print("\nü§ñ AI Assistant: ", end="", flush=True)
    for chunk in llm.stream([
        system_message,
        HumanMessage(content="Answer the question.")
    ]):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            full_response += chunk.content
    print()  # New line after streaming
    
  else:
    print("üí≠ Using conversation context...")
    human_message = HumanMessage(content=question)
    messages = messages+[human_message]
    
    # Stream the response
    print("\nü§ñ AI Assistant: ", end="", flush=True)
    for chunk in llm.stream(messages):
        if chunk.content:
            print(chunk.content, end="", flush=True)
            full_response += chunk.content
    print()  # New line after streaming

  return {
        "answer": full_response,  # get the full streamed response
        "messages": messages + [AIMessage(content=full_response)]
    }

def route_based_on_search(state) -> str:
    if state.get("needs_search"):
        return "search_web"
    else:
        return "generate_answer"

from langgraph.graph import  StateGraph,START,END
from langgraph.graph import  MessagesState
from langgraph.prebuilt import  ToolNode
from langgraph.prebuilt import  tools_condition
# from IPython.display import Image,display
from langchain_core.messages import HumanMessage ,SystemMessage
from langgraph.checkpoint.memory import MemorySaver


memory=MemorySaver()


builder = StateGraph(Researchstate)

builder.add_node("check",check)

# Initialize each node with node_secret
builder.add_node("search_web",search_web)
builder.add_node("search_wikipedia", search_wiki)
builder.add_node("generate_answer", generate_ans)

# Flow
builder.add_edge(START, "check")
builder.add_conditional_edges(
    "check",                    # the current node name
    route_based_on_search,      # your routing function
    ["search_web", "generate_answer"]  # possible destinations
)
builder.add_edge("search_web", "search_wikipedia")
builder.add_edge("search_wikipedia", "generate_answer")
builder.add_edge("generate_answer", END)
graph = builder.compile(checkpointer=memory)

# display(Image(graph.get_graph().draw_mermaid_png()))

def main():
    """Main interactive function to get user input and process questions"""
    config = {"configurable": {"thread_id": "user_session"}}
    
    print("üîç Welcome to the Research Assistant!")
    print("Ask me anything, and I'll search for information or use our conversation history.")
    print("Type 'quit', 'exit', or 'bye' to end the session.\n")
    
    while True:
        try:
            # Get user input
            user_question = input("‚ùì Your question: ").strip()
            
            # Check for exit commands
            if user_question.lower() in ['quit', 'exit', 'bye', 'q']:
                print("üëã Goodbye! Thanks for using the Research Assistant!")
                break
            
            # Skip empty questions
            if not user_question:
                print("Please enter a question.")
                continue
            
            print(f"\nü§î Processing your question: '{user_question}'")
            print("-" * 50)
            
            # Process the question through the graph
            result = graph.invoke({"question": user_question}, config=config)
            
            # Display the conversation
            # print("\nüí¨ Conversation:")
            # for message in result['messages']:
            #     message.pretty_print()
            
            print("\n" + "="*70 + "\n")
            
        except KeyboardInterrupt:
            print("\n\nüëã Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"‚ùå An error occurred: {e}")
            print("Please try again with a different question.\n")

if __name__ == "__main__":
    main()

